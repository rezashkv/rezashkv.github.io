---
title: "Deep Prompt Tuning for Graph Transformers"
category: publications
permalink: /publications/deepgpt/
excerpt: 'The quadratic complexity of self-attention operations and the extensive layering in graph transformer architectures present challenges when applying them to graph based prediction tasks'
date: 2023-09-18
venue: 'ArXiv'
paperurl: 'https://arxiv.org/abs/2309.10131'
#citation: "[Bibtex](https://scholar.googleusercontent.com/scholar.bib?q=info:IEhQosxgVZYJ:scholar.google.com/&output=citation&scisdr=ClEwYZ4DEI3rjWJld2Q:AFWwaeYAAAAAZcFjb2TxOycuzGChtDgr_6jJBXk&scisig=AFWwaeYAAAAAZcFjb9lJrPog3gEN3yiY0c5qD7U&scisf=4&ct=citation&cd=-1&hl=en)" 
---
Prompt tuning for Graph Transformers and Message Passing Graph Neural Networks, improved efficiency and resource utilization of Graph Transformers.
[View Code here](https://github.com/rezashkv/DeepGPT)

